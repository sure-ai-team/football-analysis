{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e5e5a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"ONNXRUNTIME_EXECUTION_PROVIDERS\"] = \"[CUDAExecutionProvider]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a01b4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/projects/sure-football-analysis\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "HOME = os.getcwd()\n",
    "print(HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35baa370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from inference import get_model\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# ROBOFLOW_API_KEY = os.environ.get(\"ROBOFLOW_API_KEY\")\n",
    "# PLAYER_DETECTION_MODEL_ID = \"football-players-detection-3zvbc/12\"\n",
    "# PLAYER_DETECTION_MODEL = get_model(PLAYER_DETECTION_MODEL_ID, api_key=ROBOFLOW_API_KEY)\n",
    "PLAYER_DETECTION_MODEL = YOLO(\"app/models/yolo11_football_v2/weights/best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cdafe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, SiglipVisionModel\n",
    "\n",
    "SIGLIP_MODEL_PATH = 'google/siglip-base-patch16-224'\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "EMBEDDINGS_MODEL = SiglipVisionModel.from_pretrained(SIGLIP_MODEL_PATH).to(DEVICE)\n",
    "EMBEDDINGS_PROCESSOR = AutoProcessor.from_pretrained(SIGLIP_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75e7f7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 1 ball, 2 goalkeepers, 20 players, 2 referees, 53.7ms\n",
      "Speed: 9.3ms preprocess, 53.7ms inference, 99.3ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 1it [00:00,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 1 goalkeeper, 20 players, 2 referees, 33.4ms\n",
      "Speed: 9.8ms preprocess, 33.4ms inference, 1.1ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 2it [00:01,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 1 goalkeeper, 21 players, 2 referees, 33.4ms\n",
      "Speed: 11.0ms preprocess, 33.4ms inference, 1.0ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 3it [00:01,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 20 players, 2 referees, 33.5ms\n",
      "Speed: 9.7ms preprocess, 33.5ms inference, 1.0ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 4it [00:01,  4.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 1 ball, 20 players, 2 referees, 33.2ms\n",
      "Speed: 10.6ms preprocess, 33.2ms inference, 1.0ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 5it [00:01,  4.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 1 ball, 20 players, 2 referees, 33.0ms\n",
      "Speed: 10.1ms preprocess, 33.0ms inference, 1.0ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 6it [00:01,  5.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 20 players, 2 referees, 33.2ms\n",
      "Speed: 6.8ms preprocess, 33.2ms inference, 1.4ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 7it [00:01,  6.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 1 ball, 19 players, 2 referees, 33.7ms\n",
      "Speed: 9.3ms preprocess, 33.7ms inference, 1.0ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 8it [00:01,  6.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 1 ball, 1 goalkeeper, 20 players, 2 referees, 33.0ms\n",
      "Speed: 8.1ms preprocess, 33.0ms inference, 1.0ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 9it [00:02,  6.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 1 ball, 21 players, 2 referees, 33.0ms\n",
      "Speed: 7.0ms preprocess, 33.0ms inference, 1.0ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 10it [00:02,  6.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 1 ball, 22 players, 2 referees, 33.0ms\n",
      "Speed: 9.7ms preprocess, 33.0ms inference, 1.1ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 11it [00:02,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 1 ball, 1 goalkeeper, 20 players, 2 referees, 33.1ms\n",
      "Speed: 6.9ms preprocess, 33.1ms inference, 1.0ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 12it [00:02,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 1 goalkeeper, 21 players, 2 referees, 33.1ms\n",
      "Speed: 9.9ms preprocess, 33.1ms inference, 1.0ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 13it [00:02,  7.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 1 ball, 1 goalkeeper, 20 players, 2 referees, 33.0ms\n",
      "Speed: 7.2ms preprocess, 33.0ms inference, 1.1ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 14it [00:02,  7.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 1 goalkeeper, 21 players, 2 referees, 33.2ms\n",
      "Speed: 11.3ms preprocess, 33.2ms inference, 1.1ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 15it [00:02,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 1 goalkeeper, 21 players, 2 referees, 33.0ms\n",
      "Speed: 11.2ms preprocess, 33.0ms inference, 1.5ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 16it [00:03,  6.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 20 players, 3 referees, 33.5ms\n",
      "Speed: 11.4ms preprocess, 33.5ms inference, 1.1ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 17it [00:03,  6.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 22 players, 1 referee, 33.0ms\n",
      "Speed: 11.7ms preprocess, 33.0ms inference, 1.3ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 18it [00:03,  6.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 22 players, 2 referees, 33.1ms\n",
      "Speed: 11.6ms preprocess, 33.1ms inference, 1.4ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 19it [00:03,  6.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 23 players, 2 referees, 33.4ms\n",
      "Speed: 11.3ms preprocess, 33.4ms inference, 1.0ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 20it [00:03,  6.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 1 ball, 1 goalkeeper, 19 players, 2 referees, 33.3ms\n",
      "Speed: 11.6ms preprocess, 33.3ms inference, 1.0ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 21it [00:03,  6.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 1 goalkeeper, 21 players, 2 referees, 33.2ms\n",
      "Speed: 11.1ms preprocess, 33.2ms inference, 1.0ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 22it [00:03,  6.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 1 ball, 1 goalkeeper, 21 players, 2 referees, 33.2ms\n",
      "Speed: 7.0ms preprocess, 33.2ms inference, 1.0ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 23it [00:04,  6.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 1 goalkeeper, 22 players, 1 referee, 33.3ms\n",
      "Speed: 12.6ms preprocess, 33.3ms inference, 1.3ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 24it [00:04,  6.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 1 goalkeeper, 22 players, 2 referees, 33.5ms\n",
      "Speed: 11.1ms preprocess, 33.5ms inference, 1.1ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 25it [00:04,  5.62it/s]\n",
      "embedding extraction: 8it [00:03,  2.42it/s]\n"
     ]
    }
   ],
   "source": [
    "import supervision as sv\n",
    "import numpy as np\n",
    "from more_itertools import chunked\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import torch\n",
    "\n",
    "# Suppress FutureWarnings from sklearn\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn\")\n",
    "\n",
    "# Configuration\n",
    "SOURCE_VIDEO_PATH = \"app/test_data/raw/121364_0.mp4\"\n",
    "BATCH_SIZE = 64\n",
    "PLAYER_ID = 2\n",
    "STRIDE = 30\n",
    "\n",
    "# Frame generator\n",
    "frame_generator = sv.get_video_frames_generator(\n",
    "    source_path=SOURCE_VIDEO_PATH, stride=STRIDE\n",
    ")\n",
    "\n",
    "# Collect crops\n",
    "crops = []\n",
    "for frame in tqdm(frame_generator, desc=\"collecting crops\"):\n",
    "    result = PLAYER_DETECTION_MODEL.predict(frame, conf=0.3)[0]\n",
    "    detections = sv.Detections.from_ultralytics(result)\n",
    "    detections = detections.with_nms(threshold=0.5, class_agnostic=True)\n",
    "    detections = detections[detections.class_id == PLAYER_ID]\n",
    "    players_crops = [sv.crop_image(frame, xyxy) for xyxy in detections.xyxy]\n",
    "    crops += players_crops\n",
    "\n",
    "# Convert crops to pillow format\n",
    "crops = [sv.cv2_to_pillow(crop) for crop in crops]\n",
    "\n",
    "# Process crops in batches\n",
    "batches = chunked(crops, BATCH_SIZE)\n",
    "data = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(batches, desc=\"embedding extraction\"):\n",
    "        inputs = EMBEDDINGS_PROCESSOR(images=batch, return_tensors=\"pt\").to(DEVICE)\n",
    "        outputs = EMBEDDINGS_MODEL(**inputs)\n",
    "        embeddings = torch.mean(outputs.last_hidden_state, dim=1).cpu().numpy()\n",
    "        data.append(embeddings)\n",
    "\n",
    "# Concatenate all embeddings\n",
    "data = np.concatenate(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb36e998",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 1 ball, 2 goalkeepers, 20 players, 2 referees, 33.0ms\n",
      "Speed: 12.7ms preprocess, 33.0ms inference, 1.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 1 goalkeeper, 20 players, 2 referees, 33.5ms\n",
      "Speed: 7.4ms preprocess, 33.5ms inference, 1.0ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 2it [00:00,  9.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 1 goalkeeper, 21 players, 2 referees, 32.8ms\n",
      "Speed: 6.9ms preprocess, 32.8ms inference, 1.1ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 3it [00:00,  8.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 20 players, 2 referees, 33.5ms\n",
      "Speed: 10.1ms preprocess, 33.5ms inference, 1.0ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 4it [00:00,  8.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 1 ball, 20 players, 2 referees, 33.5ms\n",
      "Speed: 10.8ms preprocess, 33.5ms inference, 1.0ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 5it [00:00,  7.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 1 ball, 20 players, 2 referees, 33.1ms\n",
      "Speed: 10.1ms preprocess, 33.1ms inference, 1.0ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 6it [00:00,  7.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 20 players, 2 referees, 33.4ms\n",
      "Speed: 7.1ms preprocess, 33.4ms inference, 1.0ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 7it [00:00,  7.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 1 ball, 19 players, 2 referees, 33.3ms\n",
      "Speed: 10.0ms preprocess, 33.3ms inference, 1.2ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 8it [00:01,  7.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 1 ball, 1 goalkeeper, 20 players, 2 referees, 33.3ms\n",
      "Speed: 7.0ms preprocess, 33.3ms inference, 1.0ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 9it [00:01,  7.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 1 ball, 21 players, 2 referees, 33.5ms\n",
      "Speed: 7.5ms preprocess, 33.5ms inference, 1.0ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 10it [00:01,  7.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 1 ball, 22 players, 2 referees, 32.9ms\n",
      "Speed: 9.9ms preprocess, 32.9ms inference, 1.3ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 11it [00:01,  7.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 1 ball, 1 goalkeeper, 20 players, 2 referees, 33.3ms\n",
      "Speed: 11.2ms preprocess, 33.3ms inference, 1.1ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 12it [00:01,  7.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 1 goalkeeper, 21 players, 2 referees, 33.1ms\n",
      "Speed: 10.1ms preprocess, 33.1ms inference, 1.0ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 13it [00:01,  7.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 1 ball, 1 goalkeeper, 20 players, 2 referees, 33.4ms\n",
      "Speed: 6.8ms preprocess, 33.4ms inference, 1.0ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 14it [00:01,  7.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 1 goalkeeper, 21 players, 2 referees, 32.9ms\n",
      "Speed: 10.6ms preprocess, 32.9ms inference, 1.0ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 15it [00:01,  7.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 1 goalkeeper, 21 players, 2 referees, 33.1ms\n",
      "Speed: 11.2ms preprocess, 33.1ms inference, 1.0ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 16it [00:02,  7.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 20 players, 3 referees, 32.9ms\n",
      "Speed: 11.1ms preprocess, 32.9ms inference, 1.0ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 17it [00:02,  7.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 22 players, 1 referee, 33.5ms\n",
      "Speed: 10.0ms preprocess, 33.5ms inference, 1.0ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 18it [00:02,  7.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 22 players, 2 referees, 33.5ms\n",
      "Speed: 11.2ms preprocess, 33.5ms inference, 1.0ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 19it [00:02,  7.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 23 players, 2 referees, 32.8ms\n",
      "Speed: 10.1ms preprocess, 32.8ms inference, 1.0ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 20it [00:02,  7.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 1 ball, 1 goalkeeper, 19 players, 2 referees, 32.9ms\n",
      "Speed: 6.8ms preprocess, 32.9ms inference, 1.2ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 21it [00:02,  7.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 1 goalkeeper, 21 players, 2 referees, 33.1ms\n",
      "Speed: 11.2ms preprocess, 33.1ms inference, 1.1ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 22it [00:02,  7.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 1 ball, 1 goalkeeper, 21 players, 2 referees, 33.3ms\n",
      "Speed: 10.7ms preprocess, 33.3ms inference, 1.2ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 23it [00:03,  7.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 1 goalkeeper, 22 players, 1 referee, 33.4ms\n",
      "Speed: 6.9ms preprocess, 33.4ms inference, 1.1ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 24it [00:03,  7.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 1 goalkeeper, 22 players, 2 referees, 33.0ms\n",
      "Speed: 6.8ms preprocess, 33.0ms inference, 1.1ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 25it [00:03,  7.42it/s]\n",
      "Embedding extraction: 19it [00:03,  4.81it/s]\n"
     ]
    }
   ],
   "source": [
    "import umap\n",
    "from sklearn.cluster import KMeans\n",
    "from sports.common.team import TeamClassifier\n",
    "\n",
    "\n",
    "REDUCER = umap.UMAP(n_components=3)\n",
    "CLUSTERING_MODEL = KMeans(n_clusters=2)\n",
    "\n",
    "projections = REDUCER.fit_transform(data)\n",
    "clusters = CLUSTERING_MODEL.fit_predict(projections)\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(\n",
    "    source_path=SOURCE_VIDEO_PATH, stride=STRIDE)\n",
    "\n",
    "crops = []\n",
    "for frame in tqdm(frame_generator, desc='collecting crops'):\n",
    "    # result = PLAYER_DETECTION_MODEL.infer(frame, confidence=0.3)[0]\n",
    "    result = PLAYER_DETECTION_MODEL.predict(frame, conf=0.3)[0]\n",
    "    # detections = sv.Detections.from_inference(result)\n",
    "    detections = sv.Detections.from_ultralytics(result)\n",
    "    players_detections = detections[detections.class_id == PLAYER_ID]\n",
    "    players_crops = [sv.crop_image(frame, xyxy) for xyxy in detections.xyxy]\n",
    "    crops += players_crops\n",
    "\n",
    "team_classifier = TeamClassifier(device=\"cuda\")\n",
    "team_classifier.fit(crops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae6e55f",
   "metadata": {},
   "source": [
    "## Imports and Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e300f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import supervision as sv\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from boxmot import BotSort # Using BoTSORT\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from collections import defaultdict, deque\n",
    "import warnings\n",
    "import logging\n",
    "import traceback # Import traceback for detailed error printing\n",
    "import os\n",
    "import random # Added for sparkle effect\n",
    "import math # Added for distance calculation\n",
    "import argparse # Added for command-line arguments\n",
    "import base64 # For encoding images for Groq API\n",
    "from groq import Groq # For interacting with Groq API\n",
    "\n",
    "# Suppress most logging messages\n",
    "logging.basicConfig(level=logging.ERROR) # Show only CRITICAL errors\n",
    "logging.disable(logging.WARNING) # Disable WARNING messages\n",
    "logging.disable(logging.INFO) # Disable INFO messages specifically\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='paddle')\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='torchvision') # Ignore potential torchvision warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning) # Ignore potential future warnings\n",
    "\n",
    "# Attempt to import PaddleOCR\n",
    "try:\n",
    "    from paddleocr import PaddleOCR\n",
    "    PADDLEOCR_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Warning: PaddleOCR not found. OCR functionality will be disabled.\")\n",
    "    PADDLEOCR_AVAILABLE = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffb2c740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Configuration -----\n",
    "# --- Paths ---\n",
    "DEFAULT_SOURCE_VIDEO_PATH = \"app/test_data/raw/your_video.mp4\" # INPUT: Path to your video\n",
    "DEFAULT_OUTPUT_VIDEO_PATH = \"your_video_tracking_actions_analyzed.mp4\" # OUTPUT: Path for annotated video\n",
    "DEFAULT_ACTIONS_DIR = \"app/test_data/predicted/action_frames\" # OUTPUT: Base directory for saved frame sequences\n",
    "OCR_DEBUG_DIR = \"ocr_debug_crops\" # Optional output for OCR debugging\n",
    "DEFAULT_REID_WEIGHTS_PATH = 'clip_market1501.pt' # INPUT: Path for BoTSORT ReID weights (if used)\n",
    "\n",
    "# --- Processing Device ---\n",
    "DEVICE = torch.device(0) if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# --- Class IDs ---\n",
    "# Initial detection model IDs (MUST match your detection model output)\n",
    "BALL_ID = 0\n",
    "GOALKEEPER_ID = 1\n",
    "PLAYER_ID = 2\n",
    "REFEREE_ID = 3\n",
    "# Team/Role Class IDs (assigned *after* classification/resolution)\n",
    "TEAM_A_ID = 0 # Example ID for Team A\n",
    "TEAM_B_ID = 1 # Example ID for Team B\n",
    "REFEREE_TEAM_ID = 2 # Example ID for Referee\n",
    "\n",
    "# --- OCR Configuration ---\n",
    "OCR_CONFIDENCE_THRESHOLD = 0.8 # Minimum confidence for accepting OCR result\n",
    "MIN_JERSEY_DIGITS = 1 # Min number of digits expected on a jersey\n",
    "MAX_JERSEY_DIGITS = 2 # Max number of digits expected on a jersey\n",
    "\n",
    "# --- ID Management Configuration ---\n",
    "LOST_TRACK_MEMORY_SECONDS = 20 # How long to remember a lost track ID for potential re-identification\n",
    "MISMATCH_CONSISTENCY_FRAMES = 3 # How many consecutive frames an OCR mismatch must occur to update ID\n",
    "\n",
    "# --- Ball Trail Configuration ---\n",
    "BALL_TRAIL_SECONDS = 1 # Duration of the visual ball trail\n",
    "SPARKLE_COUNT = 3 # Number of sparkles per trail point\n",
    "SPARKLE_RADIUS = 2 # Radius of sparkles\n",
    "SPARKLE_OFFSET = 3 # Max random offset for sparkles\n",
    "MAX_BALL_DISTANCE_PER_FRAME = 400 # Max pixels ball can move between frames (for outlier rejection)\n",
    "\n",
    "# --- Interaction Detection & Frame Saving Configuration ---\n",
    "IOU_THRESHOLD = 0.05 # Min Intersection over Union for player-ball interaction\n",
    "MIN_INTERACTION_FRAMES = 5  # Min consecutive frames for a valid interaction\n",
    "CLIP_PADDING_SECONDS = 2 # Seconds of padding before/after interaction core\n",
    "FRAMES_PER_SECOND_TO_SAVE = 5 # Target FPS for saved frame sequences (for VLM)\n",
    "\n",
    "# --- Groq API Configuration ---\n",
    "# IMPORTANT: Set the GROQ_API_KEY environment variable before running!\n",
    "# export GROQ_API_KEY='your_actual_api_key'\n",
    "GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\")\n",
    "GROQ_VLM_MODEL = \"llama3-groq-vision-alpha\" # Adjust if needed, check Groq docs for current vision models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16c50846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PaddleOCR initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "# ----- Initialize PaddleOCR -----\n",
    "ocr_model = None\n",
    "if PADDLEOCR_AVAILABLE:\n",
    "    try:\n",
    "        # Initialize PaddleOCR - adjust parameters as needed\n",
    "        ocr_model = PaddleOCR(\n",
    "            use_angle_cls=False, # Usually false for jersey numbers\n",
    "            lang='en',           # Assume English digits\n",
    "            use_gpu=(DEVICE.type == 'cuda'), # Use GPU if available\n",
    "            show_log=False       # Suppress PaddleOCR's internal logs\n",
    "        )\n",
    "        print(\"PaddleOCR initialized successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing PaddleOCR: {e}. Disabling OCR.\")\n",
    "        PADDLEOCR_AVAILABLE = False\n",
    "else:\n",
    "    print(\"PaddleOCR not available. Skipping initialization.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1d11653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Color Calculation & Helper -----\n",
    "DEFAULT_TEAM_A_COLOR = sv.Color.from_hex('#FF0000') # Red\n",
    "DEFAULT_TEAM_B_COLOR = sv.Color.from_hex('#00FFFF') # Cyan\n",
    "DEFAULT_REFEREE_COLOR = sv.Color.from_hex('#00FF00') # Green\n",
    "FALLBACK_COLOR = sv.Color.from_hex('#808080') # Grey\n",
    "COLOR_SIMILARITY_THRESHOLD = 50.0 # Max RGB distance diff to be considered ambiguous\n",
    "\n",
    "def calculate_average_color(frame: np.ndarray, detections: sv.Detections, central_fraction: float = 0.5) -> sv.Color | None:\n",
    "    \"\"\"Calculates the average color from the central region of detection boxes.\"\"\"\n",
    "    if len(detections) == 0: return None\n",
    "    avg_colors = []\n",
    "    height, width, _ = frame.shape\n",
    "    for xyxy in detections.xyxy:\n",
    "        x1, y1, x2, y2 = map(int, xyxy)\n",
    "        x1, y1 = max(0, x1), max(0, y1); x2, y2 = min(width, x2), min(height, y2)\n",
    "        if x1 >= x2 or y1 >= y2: continue # Skip invalid boxes\n",
    "        # Extract central region\n",
    "        box_w, box_h = x2 - x1, y2 - y1\n",
    "        center_x, center_y = x1 + box_w // 2, y1 + box_h // 2\n",
    "        central_w, central_h = int(box_w * central_fraction), int(box_h * central_fraction)\n",
    "        cx1 = max(x1, center_x - central_w // 2); cy1 = max(y1, center_y - central_h // 2)\n",
    "        cx2 = min(x2, center_x + central_w // 2); cy2 = min(y2, center_y + central_h // 2)\n",
    "        if cx1 >= cx2 or cy1 >= cy2: continue # Skip if central region is invalid\n",
    "        crop = frame[cy1:cy2, cx1:cx2]\n",
    "        if crop.size > 0:\n",
    "            avg_bgr = cv2.mean(crop)[:3] # Calculate mean BGR\n",
    "            avg_colors.append(avg_bgr)\n",
    "    if not avg_colors: return None\n",
    "    # Calculate overall average and convert to sv.Color\n",
    "    final_avg_bgr = np.mean(avg_colors, axis=0)\n",
    "    b, g, r = map(int, final_avg_bgr)\n",
    "    # Prevent very dark colors (optional adjustment)\n",
    "    min_intensity = 50\n",
    "    if r < min_intensity and g < min_intensity and b < min_intensity:\n",
    "        r, g, b = min_intensity, min_intensity, min_intensity\n",
    "    return sv.Color(r=r, g=g, b=b)\n",
    "\n",
    "def color_distance(color1: sv.Color | None, color2: sv.Color | None) -> float:\n",
    "    \"\"\"Calculates Euclidean distance between two sv.Color objects in RGB space.\"\"\"\n",
    "    if color1 is None or color2 is None: return float('inf')\n",
    "    # Basic check for valid sv.Color objects\n",
    "    if not all(hasattr(c, attr) for c in [color1, color2] for attr in ['r', 'g', 'b']): return float('inf')\n",
    "    try:\n",
    "        rgb1 = np.array([color1.r, color1.g, color1.b])\n",
    "        rgb2 = np.array([color2.r, color2.g, color2.b])\n",
    "        return np.linalg.norm(rgb1 - rgb2) # Euclidean distance\n",
    "    except Exception as e: print(f\"Error calculating color distance: {e}\"); return float('inf')\n",
    "\n",
    "# ----- Enhanced Goalkeeper Resolution Function -----\n",
    "def resolve_goalkeepers_team_id(\n",
    "    frame: np.ndarray,\n",
    "    goalkeepers: sv.Detections,\n",
    "    team_a_color: sv.Color | None,\n",
    "    team_b_color: sv.Color | None,\n",
    "    color_similarity_threshold: float = COLOR_SIMILARITY_THRESHOLD\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Assigns team IDs (TEAM_A_ID or TEAM_B_ID) to goalkeepers.\"\"\"\n",
    "    goalkeeper_team_ids = []\n",
    "    if len(goalkeepers) == 0: return np.array([], dtype=int)\n",
    "\n",
    "    frame_height, frame_width, _ = frame.shape\n",
    "    valid_team_colors = team_a_color is not None and team_b_color is not None\n",
    "\n",
    "    for i in range(len(goalkeepers)):\n",
    "        gk_detection_single = goalkeepers[i:i+1] # Process one GK at a time\n",
    "        # Get center x-coordinate for positional fallback\n",
    "        gk_center_x, _ = gk_detection_single.get_anchors_coordinates(sv.Position.CENTER)[0]\n",
    "        assigned_id = -1 # Default to invalid ID\n",
    "\n",
    "        # 1. Try Color Similarity if team colors are valid\n",
    "        if valid_team_colors:\n",
    "            gk_color = calculate_average_color(frame, gk_detection_single)\n",
    "            if gk_color is not None:\n",
    "                dist_a = color_distance(gk_color, team_a_color)\n",
    "                dist_b = color_distance(gk_color, team_b_color)\n",
    "                # Assign if colors are distinct enough\n",
    "                if abs(dist_a - dist_b) > color_similarity_threshold:\n",
    "                    assigned_id = TEAM_A_ID if dist_a < dist_b else TEAM_B_ID\n",
    "                # else: Color is ambiguous, proceed to fallback\n",
    "\n",
    "        # 2. Positional Fallback (if color failed or was ambiguous)\n",
    "        if assigned_id == -1:\n",
    "            # Assign based on which half of the pitch they are on\n",
    "            # Assumes Team A (ID 0) defends left goal, Team B (ID 1) defends right\n",
    "            assigned_id = TEAM_A_ID if gk_center_x < frame_width / 2 else TEAM_B_ID\n",
    "\n",
    "        goalkeeper_team_ids.append(assigned_id)\n",
    "\n",
    "    return np.array(goalkeeper_team_ids, dtype=int)\n",
    "\n",
    "\n",
    "# ----- OCR Function -----\n",
    "def perform_ocr_on_crop(crop: np.ndarray) -> tuple[str | None, float | None]:\n",
    "    \"\"\"Performs OCR on a given crop, returning the best digit sequence and confidence.\"\"\"\n",
    "    if not PADDLEOCR_AVAILABLE or ocr_model is None or crop.size == 0:\n",
    "        return None, None # Return None if OCR not available or crop is empty\n",
    "    try:\n",
    "        # Perform OCR using the initialized model\n",
    "        result = ocr_model.ocr(crop, cls=False) # cls=False might improve speed for digits\n",
    "        best_num, highest_conf = None, 0.0\n",
    "\n",
    "        # Process results: PaddleOCR typically returns a list of lists\n",
    "        # Each inner list can contain [box_coordinates, (text, confidence)]\n",
    "        if result and result[0]: # Check if result is not empty\n",
    "             for res_item in result[0]:\n",
    "                 # Ensure the structure is as expected\n",
    "                 if len(res_item) == 2 and isinstance(res_item[1], tuple) and len(res_item[1]) == 2:\n",
    "                     text, confidence = res_item[1]\n",
    "                     # Validate the extracted text and confidence\n",
    "                     if (isinstance(text, str) and text.isdigit() and\n",
    "                         MIN_JERSEY_DIGITS <= len(text) <= MAX_JERSEY_DIGITS and\n",
    "                         isinstance(confidence, (float, int)) and\n",
    "                         confidence > OCR_CONFIDENCE_THRESHOLD):\n",
    "                         # Keep the result with the highest confidence\n",
    "                         if confidence > highest_conf:\n",
    "                             highest_conf, best_num = confidence, text\n",
    "        # Return the best number found, or None if no valid number met the threshold\n",
    "        return best_num, highest_conf if best_num else None\n",
    "    except Exception as e:\n",
    "        # Log errors during OCR inference\n",
    "        print(f\"Error during PaddleOCR inference: {e}\")\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73edb73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Interaction Tracking Class -----\n",
    "class InteractionTracker:\n",
    "    \"\"\"\n",
    "    Tracks interactions between ball and players based on IoU,\n",
    "    manages confirmed interactions, and buffers frames for clip extraction.\n",
    "    \"\"\"\n",
    "    def __init__(self, fps):\n",
    "        self.fps = fps if fps > 0 else 30 # Use default FPS if invalid\n",
    "        # Stores active interactions: key=(ball_id, player_id) -> {details}\n",
    "        self.active_interactions = defaultdict(lambda: {\n",
    "            'start_frame': None, 'end_frame': None,\n",
    "            'ball_id': None, 'player_id': None, 'active': False\n",
    "        })\n",
    "        # List to store details of confirmed interactions\n",
    "        self.confirmed_interactions = []\n",
    "        # Calculate buffer size based on padding and a safety margin\n",
    "        buffer_seconds = (CLIP_PADDING_SECONDS * 2) + 5 # e.g., 2s before + 2s after + 5s margin\n",
    "        max_buffer_frames = int(buffer_seconds * self.fps) if self.fps > 0 else 150 # Default if FPS invalid\n",
    "        # Deque to efficiently store recent frames (frame_number, frame_image)\n",
    "        self.frame_buffer = deque(maxlen=max_buffer_frames)\n",
    "        # Keep track of interaction indices already processed (for saving frames)\n",
    "        self.processed_interaction_indices = set()\n",
    "\n",
    "    def update(self, frame_number, current_frame, interactions):\n",
    "        \"\"\"\n",
    "        Updates interaction status based on current frame's interactions\n",
    "        and stores the frame in the buffer.\n",
    "\n",
    "        Args:\n",
    "            frame_number (int): The current frame number.\n",
    "            current_frame (np.ndarray): The current video frame (original, pre-annotation).\n",
    "            interactions (list): List of (ball_tracker_id, player_tracker_id) tuples\n",
    "                                 representing interactions in the current frame.\n",
    "        \"\"\"\n",
    "        # Store a *copy* of the original frame to avoid modification by later annotation steps\n",
    "        self.frame_buffer.append((frame_number, current_frame.copy()))\n",
    "\n",
    "        # Set of active (ball_id, player_id) keys in the current frame\n",
    "        current_interaction_keys = set(interactions)\n",
    "\n",
    "        # --- Update Active Interactions ---\n",
    "        for ball_id, player_id in current_interaction_keys:\n",
    "            key = (ball_id, player_id)\n",
    "            if not self.active_interactions[key]['active']:\n",
    "                # Start tracking a new potential interaction\n",
    "                self.active_interactions[key] = {\n",
    "                    'start_frame': frame_number, 'end_frame': frame_number,\n",
    "                    'ball_id': ball_id, 'player_id': player_id, 'active': True\n",
    "                }\n",
    "            else:\n",
    "                # Continue an existing interaction, update the end frame\n",
    "                self.active_interactions[key]['end_frame'] = frame_number\n",
    "\n",
    "        # --- Check for Ended Interactions ---\n",
    "        # Find keys that were active previously but are not in the current frame\n",
    "        ended_keys = [k for k in self.active_interactions\n",
    "                      if self.active_interactions[k]['active'] and k not in current_interaction_keys]\n",
    "\n",
    "        for key in ended_keys:\n",
    "            interaction_data = self.active_interactions[key]\n",
    "            # Mark as inactive *before* checking duration\n",
    "            self.active_interactions[key]['active'] = False\n",
    "\n",
    "            # Calculate interaction duration (inclusive)\n",
    "            duration = (interaction_data['end_frame'] - interaction_data['start_frame']) + 1\n",
    "\n",
    "            # Check if the interaction meets the minimum duration threshold\n",
    "            if duration >= MIN_INTERACTION_FRAMES:\n",
    "                # Calculate padding in frames\n",
    "                padding_frames = int(CLIP_PADDING_SECONDS * self.fps) if self.fps > 0 else 0\n",
    "                # Calculate clip start/end frames including padding\n",
    "                clip_start_frame = max(0, interaction_data['start_frame'] - padding_frames)\n",
    "                # Pad *after* the last frame the interaction was seen\n",
    "                clip_end_frame = interaction_data['end_frame'] + padding_frames\n",
    "\n",
    "                # Store the confirmed interaction details\n",
    "                self.confirmed_interactions.append({\n",
    "                    'ball_id': interaction_data['ball_id'],\n",
    "                    'player_id': interaction_data['player_id'],\n",
    "                    'interaction_start_frame': interaction_data['start_frame'],\n",
    "                    'interaction_end_frame': interaction_data['end_frame'],\n",
    "                    'clip_start_frame': clip_start_frame,\n",
    "                    'clip_end_frame': clip_end_frame,\n",
    "                })\n",
    "                # Optional: Log confirmed interaction\n",
    "                # print(f\"Confirmed interaction: Ball {interaction_data['ball_id']} / Player {interaction_data['player_id']} \"\n",
    "                #       f\"(Frames {interaction_data['start_frame']}-{interaction_data['end_frame']}, Duration: {duration} frames)\")\n",
    "\n",
    "    def get_clip_frames_data(self, interaction_index):\n",
    "        \"\"\"\n",
    "        Retrieves frame numbers and frame images for a specific confirmed interaction,\n",
    "        ensuring it hasn't been processed before for saving.\n",
    "\n",
    "        Args:\n",
    "            interaction_index (int): The index of the confirmed interaction.\n",
    "\n",
    "        Returns:\n",
    "            list or None: A list of (frame_number, frame_image) tuples for the clip,\n",
    "                          or None if the index is invalid or already processed.\n",
    "        \"\"\"\n",
    "        # Check if index is valid and not already processed\n",
    "        if interaction_index in self.processed_interaction_indices or \\\n",
    "           interaction_index >= len(self.confirmed_interactions):\n",
    "             return None # Invalid index or already handled\n",
    "\n",
    "        interaction = self.confirmed_interactions[interaction_index]\n",
    "        start_frame = interaction['clip_start_frame']\n",
    "        end_frame = interaction['clip_end_frame']\n",
    "\n",
    "        # Extract relevant frames from the buffer based on calculated clip range\n",
    "        clip_frames_data = [(f_num, frame) for (f_num, frame) in self.frame_buffer\n",
    "                           if start_frame <= f_num <= end_frame]\n",
    "\n",
    "        # Handle cases where buffer might not contain all needed frames (e.g., very long interactions)\n",
    "        if not clip_frames_data:\n",
    "            print(f\"Warning: No frames found in buffer for interaction {interaction_index} \"\n",
    "                  f\"(Requested frames {start_frame}-{end_frame}). Buffer might be too small \"\n",
    "                  f\"or interaction happened too long ago.\")\n",
    "            # Mark as processed even if no frames found to avoid retrying\n",
    "            self.processed_interaction_indices.add(interaction_index)\n",
    "            return None\n",
    "\n",
    "        # Mark this interaction index as processed to prevent duplicate saving/analysis\n",
    "        self.processed_interaction_indices.add(interaction_index)\n",
    "        return clip_frames_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf650d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Frame Saving Function -----\n",
    "def save_interaction_frames(clip_frames_data, base_dir, interaction_info, original_fps) -> str | None:\n",
    "    \"\"\"\n",
    "    Saves selected frames from clip data as images, sampled at target FPS.\n",
    "\n",
    "    Args:\n",
    "        clip_frames_data (list): List of (frame_number, frame_image) tuples.\n",
    "        base_dir (str): The root directory to save action frame sequences.\n",
    "        interaction_info (dict): Dictionary containing 'index', 'ball_id', 'player_id'.\n",
    "        original_fps (float): The FPS of the source video.\n",
    "\n",
    "    Returns:\n",
    "        str | None: The path to the directory where frames were saved, or None on failure.\n",
    "    \"\"\"\n",
    "    if not clip_frames_data:\n",
    "        # print(f\"Info: No frames to save for interaction {interaction_info.get('index', 'N/A')}.\")\n",
    "        return None # Nothing to save\n",
    "\n",
    "    # Create a unique directory name for this interaction clip sequence\n",
    "    clip_dir_name = (f\"action_{interaction_info.get('index', 'unk'):04d}_\"\n",
    "                     f\"b{interaction_info.get('ball_id', 'unk')}_\"\n",
    "                     f\"p{interaction_info.get('player_id', 'unk')}\")\n",
    "    clip_path = os.path.join(base_dir, clip_dir_name)\n",
    "    try:\n",
    "        # Create the directory, including parent directories if needed\n",
    "        os.makedirs(clip_path, exist_ok=True)\n",
    "    except OSError as e:\n",
    "        print(f\"Error creating directory {clip_path}: {e}\")\n",
    "        return None # Cannot save frames if directory creation fails\n",
    "\n",
    "    # Calculate the frame step needed to achieve the target save FPS\n",
    "    if FRAMES_PER_SECOND_TO_SAVE <= 0 or original_fps <= 0:\n",
    "        frame_step = 1 # Save all frames if target/original FPS is invalid\n",
    "        # print(\"Warning: Saving all frames for clip due to invalid target/original FPS.\")\n",
    "    else:\n",
    "        # Ensure step is at least 1\n",
    "        frame_step = max(1, round(original_fps / FRAMES_PER_SECOND_TO_SAVE))\n",
    "\n",
    "    saved_count = 0\n",
    "    # Iterate through the clip frames using the calculated step\n",
    "    for i in range(0, len(clip_frames_data), frame_step):\n",
    "        frame_number, frame_image = clip_frames_data[i]\n",
    "        # Use the original frame number in the filename for traceability\n",
    "        frame_filename = f\"frame_{frame_number:06d}.jpg\" # Using jpg for potentially smaller size\n",
    "        frame_filepath = os.path.join(clip_path, frame_filename)\n",
    "\n",
    "        try:\n",
    "            # Save the frame as a JPG image with decent quality\n",
    "            cv2.imwrite(frame_filepath, frame_image, [int(cv2.IMWRITE_JPEG_QUALITY), 90])\n",
    "            saved_count += 1\n",
    "        except Exception as e:\n",
    "            # Log error but continue trying to save other frames\n",
    "            print(f\"Error saving frame {frame_number} to {frame_filepath}: {e}\")\n",
    "\n",
    "    if saved_count > 0:\n",
    "        # print(f\"Saved {saved_count} frames for interaction {interaction_info.get('index', 'N/A')} \"\n",
    "        #       f\"to {clip_path} (saved every {frame_step} frame(s))\")\n",
    "        return clip_path # Return the path where frames were saved\n",
    "    else:\n",
    "         print(f\"Info: No frames were successfully saved for interaction {interaction_info.get('index', 'N/A')}\")\n",
    "         # Optionally remove the empty directory\n",
    "         try: os.rmdir(clip_path)\n",
    "         except OSError: pass\n",
    "         return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f4bb7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Groq API Interaction Functions -----\n",
    "\n",
    "def encode_image(image_path: str) -> str | None:\n",
    "    \"\"\"Encodes a single image file to a base64 string.\"\"\"\n",
    "    try:\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Image file not found at {image_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error encoding image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def analyze_action_frames(clip_dir: str, groq_client: Groq, player_id: int | str, ball_id: int | str):\n",
    "    \"\"\"\n",
    "    Sends frames from a directory to Groq VLM and prints the analysis.\n",
    "\n",
    "    Args:\n",
    "        clip_dir (str): Path to the directory containing saved frame images (.jpg).\n",
    "        groq_client (Groq): Initialized Groq API client.\n",
    "        player_id (int | str): The tracker ID of the player involved in the interaction.\n",
    "        ball_id (int | str): The tracker ID (or placeholder) of the ball involved.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Analyzing action for Player {player_id} (Interaction Dir: {os.path.basename(clip_dir)}) ---\")\n",
    "    try:\n",
    "        image_files = sorted([os.path.join(clip_dir, f) for f in os.listdir(clip_dir) if f.lower().endswith(\".jpg\")])\n",
    "        if not image_files:\n",
    "            print(\"No image files found in the directory.\")\n",
    "            return\n",
    "\n",
    "        # Prepare message content with text prompt and image URLs\n",
    "        content = [{\"type\": \"text\", \"text\": f\"Describe the action performed by the player (Tracker ID: {player_id}) interacting with the ball (ID: {ball_id}) in this sequence of frames.\"}]\n",
    "        print(f\"Sending {len(image_files)} frames to Groq VLM...\")\n",
    "\n",
    "        for image_path in image_files:\n",
    "            base64_image = encode_image(image_path)\n",
    "            if base64_image:\n",
    "                content.append({\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"},\n",
    "                })\n",
    "            else:\n",
    "                print(f\"Skipping invalid image: {image_path}\")\n",
    "\n",
    "        if len(content) <= 1: # Only text prompt, no valid images\n",
    "             print(\"No valid images could be encoded for analysis.\")\n",
    "             return\n",
    "\n",
    "        # Make the API call\n",
    "        chat_completion = groq_client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": content}],\n",
    "            model=GROQ_VLM_MODEL,\n",
    "            # max_tokens=1024, # Optional: Adjust token limit if needed\n",
    "            # temperature=0.2 # Optional: Adjust temperature for creativity vs factuality\n",
    "        )\n",
    "\n",
    "        # Print the response\n",
    "        analysis = chat_completion.choices[0].message.content\n",
    "        print(f\"Groq VLM Analysis:\\n{analysis}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Groq API call for {clip_dir}: {e}\")\n",
    "        traceback.print_exc()\n",
    "    print(\"--- Analysis complete ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75bd531f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Annotation Parameters -----\n",
    "ELLIPSE_THICKNESS = 1\n",
    "LABEL_TEXT_COLOR = sv.Color.BLACK\n",
    "LABEL_TEXT_POSITION = sv.Position.BOTTOM_CENTER\n",
    "LABEL_TEXT_SCALE = 0.4\n",
    "LABEL_TEXT_THICKNESS = 1\n",
    "BALL_TRAIL_BASE_COLOR = (0, 255, 255) # Bright Cyan (BGR)\n",
    "BALL_TRAIL_THICKNESS = 1\n",
    "SPARKLE_BASE_INTENSITY = 150\n",
    "SPARKLE_MAX_INTENSITY = 255\n",
    "CURRENT_BALL_MARKER_RADIUS = 4\n",
    "CURRENT_BALL_MARKER_COLOR = (255, 255, 255) # White (BGR)\n",
    "CURRENT_BALL_MARKER_THICKNESS = -1 # Filled circle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76a8dba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Global State -----\n",
    "player_data = {} # Stores data per track_id: {jersey_id, confidence, last_seen, team_id, mismatch_history}\n",
    "recently_lost_jerseys = defaultdict(lambda: deque(maxlen=10)) # jersey_num -> list of lost track info\n",
    "ball_positions = None # Deque for ball trail positions (will be initialized in main)\n",
    "\n",
    "# ----- Frame Processing Function -----\n",
    "def process_frame(\n",
    "    frame: np.ndarray,\n",
    "    frame_idx: int,\n",
    "    tracker: BotSort,\n",
    "    interaction_tracker: InteractionTracker,\n",
    "    width: int, # Pass frame dimensions\n",
    "    height: int # Pass frame dimensions\n",
    "    ):\n",
    "    \"\"\"Processes a single frame for detection, tracking, OCR, interaction, and annotation.\"\"\"\n",
    "    # Access global state (or pass models/state as arguments if preferred)\n",
    "    global player_data, recently_lost_jerseys, ball_positions, detection_model, team_classifier\n",
    "\n",
    "    # 1. Detection\n",
    "    results = detection_model.predict(frame, conf=0.3, iou=0.5, device=DEVICE, verbose=False)\n",
    "    if not results or len(results) == 0:\n",
    "        interaction_tracker.update(frame_idx, frame, []) # Update tracker even if no detections\n",
    "        return frame # Return original frame\n",
    "\n",
    "    detections = sv.Detections.from_ultralytics(results[0])\n",
    "\n",
    "    # 2. Pre-processing & Ball Position Update\n",
    "    ball_detections = detections[detections.class_id == BALL_ID]\n",
    "    people_detections = detections[detections.class_id != BALL_ID]\n",
    "\n",
    "    # Update ball trail deque (check if initialized)\n",
    "    if ball_positions is not None and len(ball_detections) > 0 :\n",
    "        x1, y1, x2, y2 = ball_detections.xyxy[0] # Assume single ball\n",
    "        current_center = (int((x1 + x2) / 2), int((y1 + y2) / 2))\n",
    "        is_valid_position = True # Check for outliers\n",
    "        if len(ball_positions) > 0:\n",
    "            prev_center = ball_positions[-1]\n",
    "            if isinstance(prev_center, tuple) and len(prev_center) == 2:\n",
    "                distance = math.dist(current_center, prev_center)\n",
    "                if distance > MAX_BALL_DISTANCE_PER_FRAME: is_valid_position = False\n",
    "        if is_valid_position: ball_positions.append(current_center)\n",
    "\n",
    "    # 3. Team/Role Classification\n",
    "    players_detections = people_detections[people_detections.class_id == PLAYER_ID]\n",
    "    goalkeepers_detections = people_detections[people_detections.class_id == GOALKEEPER_ID]\n",
    "    referees_detections = people_detections[people_detections.class_id == REFEREE_ID]\n",
    "\n",
    "    # --- Player Classification ---\n",
    "    classified_players = sv.Detections.empty()\n",
    "    if len(players_detections) > 0:\n",
    "        players_crops = []; valid_indices = []\n",
    "        for i, xyxy in enumerate(players_detections.xyxy):\n",
    "            crop = sv.crop_image(frame, xyxy);\n",
    "            if crop is not None and crop.size > 0: players_crops.append(crop); valid_indices.append(i)\n",
    "        if players_crops:\n",
    "            predicted_team_ids = team_classifier.predict(players_crops)\n",
    "            if predicted_team_ids is not None and len(predicted_team_ids) == len(players_crops):\n",
    "                assigned_ids = np.full(len(players_detections), -1, dtype=int) # Default -1\n",
    "                for i, pred_id in enumerate(predicted_team_ids): assigned_ids[valid_indices[i]] = pred_id\n",
    "                valid_classification_mask = (assigned_ids != -1) # Filter out failed classifications\n",
    "                players_detections.class_id = assigned_ids # Update class IDs\n",
    "                classified_players = players_detections[valid_classification_mask]\n",
    "\n",
    "    # --- Calculate Dynamic Team Colors ---\n",
    "    team_a_detections = classified_players[classified_players.class_id == TEAM_A_ID]\n",
    "    team_b_detections = classified_players[classified_players.class_id == TEAM_B_ID]\n",
    "    referee_only_detections = referees_detections # Use original referee detections for color calculation\n",
    "    current_team_a_color = calculate_average_color(frame, team_a_detections) or DEFAULT_TEAM_A_COLOR\n",
    "    current_team_b_color = calculate_average_color(frame, team_b_detections) or DEFAULT_TEAM_B_COLOR\n",
    "    current_referee_color = calculate_average_color(frame, referee_only_detections) or DEFAULT_REFEREE_COLOR\n",
    "    # Map final team/role IDs to their current average color\n",
    "    dynamic_color_map = {\n",
    "        TEAM_A_ID: current_team_a_color,\n",
    "        TEAM_B_ID: current_team_b_color,\n",
    "        REFEREE_TEAM_ID: current_referee_color\n",
    "    }\n",
    "\n",
    "    # --- Goalkeeper Classification ---\n",
    "    classified_gks = sv.Detections.empty()\n",
    "    if len(goalkeepers_detections) > 0:\n",
    "        gk_team_ids = resolve_goalkeepers_team_id(frame, goalkeepers_detections, current_team_a_color, current_team_b_color)\n",
    "        if gk_team_ids is not None and len(gk_team_ids) == len(goalkeepers_detections):\n",
    "            valid_gk_mask = (gk_team_ids != -1) # Ensure resolution was successful\n",
    "            goalkeepers_detections.class_id = gk_team_ids # Update class IDs to TEAM_A_ID or TEAM_B_ID\n",
    "            classified_gks = goalkeepers_detections[valid_gk_mask]\n",
    "\n",
    "    # --- Referee Classification ---\n",
    "    classified_refs = sv.Detections.empty()\n",
    "    if len(referees_detections) > 0:\n",
    "        ref_team_ids = np.full(len(referees_detections), REFEREE_TEAM_ID) # Assign specific referee ID\n",
    "        referees_detections.class_id = ref_team_ids\n",
    "        classified_refs = referees_detections\n",
    "\n",
    "    # --- Merge Detections for Tracking ---\n",
    "    # Combine all classified entities (players, GKs, refs) before sending to tracker\n",
    "    detections_to_track = sv.Detections.merge([classified_players, classified_gks, classified_refs])\n",
    "\n",
    "    # 4. Tracking using BoTSORT\n",
    "    tracked_people = sv.Detections.empty() # Detections with tracker IDs assigned\n",
    "    current_frame_tracker_ids = set() # Keep track of IDs present in this frame\n",
    "    if len(detections_to_track) > 0 and tracker is not None:\n",
    "        # Prepare input for BoTSORT: [x1, y1, x2, y2, conf, cls]\n",
    "        # Use the *final* classified IDs (TEAM_A_ID, TEAM_B_ID, REFEREE_TEAM_ID)\n",
    "        boxmot_input = np.hstack((\n",
    "            detections_to_track.xyxy,\n",
    "            detections_to_track.confidence[:, np.newaxis],\n",
    "            detections_to_track.class_id[:, np.newaxis]\n",
    "        ))\n",
    "        try:\n",
    "            # Update tracker state\n",
    "            tracks = tracker.update(boxmot_input, frame) # Pass original frame for ReID\n",
    "            if tracks.shape[0] > 0:\n",
    "                # Process tracker output: [x1, y1, x2, y2, track_id, conf, cls, ...]\n",
    "                tracked_people = sv.Detections(\n",
    "                    xyxy=tracks[:, 0:4],\n",
    "                    tracker_id=tracks[:, 4].astype(int),\n",
    "                    confidence=tracks[:, 5],\n",
    "                    class_id=tracks[:, 6].astype(int) # Use the class ID returned by the tracker\n",
    "                )\n",
    "                current_frame_tracker_ids = set(tracked_people.tracker_id)\n",
    "        except Exception as e: print(f\"[Frame {frame_idx}] Error during tracker update: {e}\")\n",
    "    elif tracker is not None: # Update tracker even if no detections this frame to advance internal state\n",
    "         try: tracker.update(np.empty((0, 6)), frame) # Empty update\n",
    "         except Exception as e: print(f\"[Frame {frame_idx}] Error updating tracker with empty input: {e}\")\n",
    "\n",
    "    # --- Use DETECTED ball for interaction checks (simpler than tracking ball) ---\n",
    "    tracked_ball = ball_detections\n",
    "\n",
    "    # 5. Interaction Detection (using TRACKED people and DETECTED ball)\n",
    "    current_interactions = [] # List of (ball_id, player_id) tuples for this frame\n",
    "    placeholder_ball_id = -1 # Use a placeholder as ball isn't tracked with persistent ID here\n",
    "\n",
    "    if len(tracked_ball) > 0 and len(tracked_people) > 0:\n",
    "        ball_box = tracked_ball.xyxy[0:1] # Get the first detected ball's box [1, 4]\n",
    "        # Consider only players/GKs for interaction (filter out referees)\n",
    "        player_mask = np.isin(tracked_people.class_id, [TEAM_A_ID, TEAM_B_ID])\n",
    "        interacting_players = tracked_people[player_mask]\n",
    "\n",
    "        if len(interacting_players) > 0:\n",
    "            # Calculate IoU between the ball and all potential players\n",
    "            iou_matrix = sv.box_iou_batch(ball_box, interacting_players.xyxy) # Shape: (1, num_players)\n",
    "            # Find indices of players whose IoU with the ball exceeds the threshold\n",
    "            interacting_player_indices = np.where(iou_matrix[0] > IOU_THRESHOLD)[0]\n",
    "\n",
    "            # Record interactions using player's tracker ID\n",
    "            for player_idx in interacting_player_indices:\n",
    "                player_tracker_id = interacting_players.tracker_id[player_idx]\n",
    "                current_interactions.append((placeholder_ball_id, player_tracker_id))\n",
    "\n",
    "    # 6. Update Interaction Tracker (Pass ORIGINAL frame before annotation)\n",
    "    interaction_tracker.update(frame_idx, frame, current_interactions)\n",
    "\n",
    "    # 7. OCR and Player ID Management (Label Generation)\n",
    "    final_labels = [] # List to store display labels for each tracked person\n",
    "    current_player_data = {} # Stores updated player data for this frame\n",
    "\n",
    "    if len(tracked_people) > 0:\n",
    "        # Get frame dimensions (needed for cropping checks)\n",
    "        frame_height, frame_width, _ = frame.shape\n",
    "\n",
    "        for i in range(len(tracked_people)):\n",
    "            track_id = tracked_people.tracker_id[i]\n",
    "            team_id = tracked_people.class_id[i] # Final team/role ID from tracker\n",
    "            bbox = tracked_people.xyxy[i]\n",
    "            # Ensure bbox coordinates are valid integers within frame bounds\n",
    "            x1, y1, x2, y2 = map(int, bbox)\n",
    "            x1, y1 = max(0, x1), max(0, y1)\n",
    "            x2, y2 = min(frame_width, x2), min(frame_height, y2) # Use frame dimensions\n",
    "\n",
    "            detected_jersey_num, ocr_confidence = None, None\n",
    "            # Attempt OCR only if bbox is valid and OCR is available\n",
    "            if x1 < x2 and y1 < y2 and PADDLEOCR_AVAILABLE:\n",
    "                 player_crop = frame[y1:y2, x1:x2]\n",
    "                 if player_crop.size > 0: # Check crop is not empty\n",
    "                    # Convert crop to grayscale for potentially better OCR\n",
    "                    gray_crop = cv2.cvtColor(player_crop, cv2.COLOR_BGR2GRAY)\n",
    "                    # Apply some basic preprocessing (optional, might help OCR)\n",
    "                    # gray_crop = cv2.threshold(gray_crop, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "                    # gray_crop = cv2.medianBlur(gray_crop, 3)\n",
    "                    detected_jersey_num, ocr_confidence = perform_ocr_on_crop(gray_crop)\n",
    "                    # --- Optional: Save debug crops ---\n",
    "                    # if detected_jersey_num is not None:\n",
    "                    #     try:\n",
    "                    #         debug_filename = os.path.join(OCR_DEBUG_DIR, f\"f{frame_idx}_t{track_id}_ocr_{detected_jersey_num}.png\")\n",
    "                    #         cv2.imwrite(debug_filename, gray_crop)\n",
    "                    #     except Exception as write_e: print(f\"Error saving OCR debug crop: {write_e}\")\n",
    "\n",
    "            # --- Update player data based on OCR result and history ---\n",
    "            assigned_jersey_id = None # Jersey ID to display for this track in this frame\n",
    "            if track_id in player_data:\n",
    "                # Track already exists, update its data\n",
    "                p_data = player_data[track_id]\n",
    "                p_data[\"last_seen\"] = frame_idx\n",
    "                p_data[\"team_id\"] = team_id # Update team ID (usually shouldn't change)\n",
    "                current_jersey_id = p_data[\"jersey_id\"] # Stored jersey ID\n",
    "                mismatch_history = p_data[\"mismatch_history\"]\n",
    "\n",
    "                if detected_jersey_num is not None:\n",
    "                    # OCR successful this frame\n",
    "                    if current_jersey_id is None or detected_jersey_num == current_jersey_id:\n",
    "                        # First time seeing number, or it matches the stored one\n",
    "                        p_data[\"jersey_id\"] = detected_jersey_num\n",
    "                        p_data[\"jersey_confidence\"] = ocr_confidence\n",
    "                        mismatch_history.clear() # Reset mismatch counter\n",
    "                    else:\n",
    "                        # Mismatch detected! Current OCR differs from stored ID\n",
    "                        mismatch_history.append(detected_jersey_num)\n",
    "                        # Check if mismatch is consistent over several frames\n",
    "                        if len(mismatch_history) >= MISMATCH_CONSISTENCY_FRAMES and all(num == detected_jersey_num for num in mismatch_history):\n",
    "                            # Consistent mismatch, update the stored jersey ID\n",
    "                            # print(f\"[Frame {frame_idx}] Track {track_id}: Jersey ID updated from {current_jersey_id} to {detected_jersey_num} (Consistent mismatch).\")\n",
    "                            p_data[\"jersey_id\"] = detected_jersey_num\n",
    "                            p_data[\"jersey_confidence\"] = ocr_confidence\n",
    "                            mismatch_history.clear() # Reset after update\n",
    "                        # else: Inconsistent mismatch, keep stored ID for now\n",
    "                else:\n",
    "                    # OCR failed this frame, clear mismatch history\n",
    "                    mismatch_history.clear()\n",
    "                assigned_jersey_id = p_data[\"jersey_id\"] # Use the potentially updated stored ID for display\n",
    "                current_player_data[track_id] = p_data # Store updated data for this frame\n",
    "\n",
    "            else:\n",
    "                # New track ID encountered\n",
    "                found_match = False\n",
    "                # Try to match with recently lost tracks using jersey number (if OCR succeeded)\n",
    "                if detected_jersey_num is not None and detected_jersey_num in recently_lost_jerseys:\n",
    "                    potential_matches = []\n",
    "                    # Check lost tracks with the same jersey number\n",
    "                    for lost_track_info in reversed(recently_lost_jerseys[detected_jersey_num]):\n",
    "                        time_diff = frame_idx - lost_track_info[\"last_seen\"]\n",
    "                        # Check if within time window and team ID matches\n",
    "                        if time_diff < LOST_TRACK_MEMORY_FRAMES and lost_track_info[\"team_id\"] == team_id:\n",
    "                            potential_matches.append((lost_track_info, time_diff))\n",
    "\n",
    "                    if potential_matches:\n",
    "                        # Found potential matches, pick the most recent one\n",
    "                        potential_matches.sort(key=lambda x: x[1]) # Sort by time difference\n",
    "                        best_match_info, _ = potential_matches[0]\n",
    "                        assigned_jersey_id = detected_jersey_num\n",
    "                        # Re-establish player data using the matched info\n",
    "                        p_data = {\n",
    "                            \"jersey_id\": assigned_jersey_id, \"jersey_confidence\": ocr_confidence,\n",
    "                            \"last_seen\": frame_idx, \"team_id\": team_id,\n",
    "                            \"mismatch_history\": deque(maxlen=MISMATCH_CONSISTENCY_FRAMES) # Reset history\n",
    "                        }\n",
    "                        current_player_data[track_id] = p_data # Assign data to the *new* track ID\n",
    "                        # print(f\"[Frame {frame_idx}] Track {track_id}: Re-identified Jersey #{assigned_jersey_id} (was lost track {best_match_info['tracker_id']}).\")\n",
    "                        # Remove the matched entry from recently_lost list\n",
    "                        try: recently_lost_jerseys[detected_jersey_num].remove(best_match_info)\n",
    "                        except ValueError: pass # Ignore if already removed\n",
    "                        found_match = True\n",
    "\n",
    "                if not found_match:\n",
    "                    # No match found, create a new player data entry for this track ID\n",
    "                    assigned_jersey_id = detected_jersey_num # Could be None if OCR failed\n",
    "                    current_player_data[track_id] = {\n",
    "                        \"jersey_id\": assigned_jersey_id,\n",
    "                        \"jersey_confidence\": ocr_confidence if detected_jersey_num is not None else None,\n",
    "                        \"last_seen\": frame_idx, \"team_id\": team_id,\n",
    "                        \"mismatch_history\": deque(maxlen=MISMATCH_CONSISTENCY_FRAMES)\n",
    "                    }\n",
    "\n",
    "            # --- Generate Display Label ---\n",
    "            # Determine team prefix based on final assigned team/role ID\n",
    "            if team_id == TEAM_A_ID: team_prefix = \"T1\"\n",
    "            elif team_id == TEAM_B_ID: team_prefix = \"T2\"\n",
    "            elif team_id == REFEREE_TEAM_ID: team_prefix = \"Ref\"\n",
    "            else: team_prefix = f\"T{team_id}\" # Fallback for unexpected IDs\n",
    "\n",
    "            base_label = f\"{team_prefix} P{track_id}\" # Base label: Team P<TrackID>\n",
    "            display_id = base_label\n",
    "            # Add jersey number if available and assigned\n",
    "            if assigned_jersey_id is not None:\n",
    "                display_id = f\"{base_label} #{assigned_jersey_id}\"\n",
    "\n",
    "            final_labels.append(display_id) # Add the final label for this track\n",
    "\n",
    "    # 8. Update Global Player Data & Handle Lost Tracks\n",
    "    # Find tracks that were present in the previous frame but not this one\n",
    "    lost_tracker_ids = set(player_data.keys()) - current_frame_tracker_ids\n",
    "    for lost_id in lost_tracker_ids:\n",
    "        lost_info = player_data[lost_id]\n",
    "        # If the lost track had a known jersey ID, add it to the recently lost list\n",
    "        # This allows matching if it reappears soon with the same number\n",
    "        if lost_info.get(\"jersey_id\") is not None:\n",
    "            recently_lost_jerseys[lost_info[\"jersey_id\"]].append({\n",
    "                \"tracker_id\": lost_id, # Store the original tracker ID that was lost\n",
    "                \"last_seen\": lost_info[\"last_seen\"],\n",
    "                \"team_id\": lost_info[\"team_id\"]\n",
    "            })\n",
    "\n",
    "    # Periodic cleanup of very old entries in recently_lost_jerseys (e.g., every minute)\n",
    "    # This prevents the dictionary from growing indefinitely\n",
    "    current_fps = interaction_tracker.fps # Get FPS from interaction tracker\n",
    "    if current_fps > 0 and frame_idx > 0 and frame_idx % (int(current_fps) * 60) == 0:\n",
    "        # print(f\"[Frame {frame_idx}] Cleaning up old lost tracks...\")\n",
    "        for jersey_num in list(recently_lost_jerseys.keys()): # Iterate over keys copy\n",
    "            q = recently_lost_jerseys[jersey_num]\n",
    "            # Keep only entries seen within twice the memory window (generous buffer)\n",
    "            valid_entries = deque([entry for entry in q if (frame_idx - entry[\"last_seen\"]) < LOST_TRACK_MEMORY_FRAMES * 2], maxlen=10)\n",
    "            if valid_entries:\n",
    "                recently_lost_jerseys[jersey_num] = valid_entries\n",
    "            else:\n",
    "                # Remove jersey number if no recent tracks associated with it\n",
    "                del recently_lost_jerseys[jersey_num]\n",
    "\n",
    "    # Update the global player data state with the data processed in this frame\n",
    "    player_data = current_player_data\n",
    "\n",
    "    # 9. Annotation\n",
    "    annotated_frame = frame.copy() # Start annotation on a clean copy of the original frame\n",
    "\n",
    "    # --- Annotate \"Magical\" Ball Trail ---\n",
    "    if ball_positions is not None and len(ball_positions) >= 2:\n",
    "        num_points = len(ball_positions)\n",
    "        for i in range(1, num_points):\n",
    "            pt1 = ball_positions[i-1]; pt2 = ball_positions[i]\n",
    "            # Ensure points are valid tuples before drawing\n",
    "            if isinstance(pt1, tuple) and isinstance(pt2, tuple) and len(pt1)==2 and len(pt2)==2:\n",
    "                 # Draw the trail line\n",
    "                 cv2.line(annotated_frame, pt1, pt2, BALL_TRAIL_BASE_COLOR, BALL_TRAIL_THICKNESS)\n",
    "                 # --- Add Sparkles (Optional visual flair) ---\n",
    "                 alpha_fraction = (i - 1) / max(1, num_points - 1) # Fade effect for sparkles\n",
    "                 sparkle_intensity = int(SPARKLE_BASE_INTENSITY + (SPARKLE_MAX_INTENSITY - SPARKLE_BASE_INTENSITY) * alpha_fraction)\n",
    "                 sparkle_color = (sparkle_intensity, sparkle_intensity, sparkle_intensity) # Grey sparkles\n",
    "                 for _ in range(SPARKLE_COUNT):\n",
    "                     # Add random offset to the end point of the line segment\n",
    "                     offset_x = random.randint(-SPARKLE_OFFSET, SPARKLE_OFFSET)\n",
    "                     offset_y = random.randint(-SPARKLE_OFFSET, SPARKLE_OFFSET)\n",
    "                     sparkle_pt = (pt2[0] + offset_x, pt2[1] + offset_y)\n",
    "                     # Draw small filled circle as a sparkle\n",
    "                     cv2.circle(annotated_frame, sparkle_pt, SPARKLE_RADIUS, sparkle_color, -1)\n",
    "\n",
    "    # --- Annotate Current Ball Position ---\n",
    "    if ball_positions is not None and len(ball_positions) > 0:\n",
    "         last_pos = ball_positions[-1] # Get the most recent position\n",
    "         if isinstance(last_pos, tuple) and len(last_pos)==2:\n",
    "              # Draw a marker (e.g., white filled circle) at the current ball position\n",
    "              cv2.circle(annotated_frame, last_pos, CURRENT_BALL_MARKER_RADIUS, CURRENT_BALL_MARKER_COLOR, CURRENT_BALL_MARKER_THICKNESS)\n",
    "\n",
    "    # --- Annotate Tracked People (Ellipses and Labels) ---\n",
    "    # Ensure we have the same number of labels as tracked people\n",
    "    if len(tracked_people) > 0 and len(final_labels) == len(tracked_people):\n",
    "        # Group by team ID for consistent color annotation per team/role\n",
    "        unique_team_ids = np.unique(tracked_people.class_id)\n",
    "        for current_team_id in unique_team_ids:\n",
    "            # Create mask for current team/role\n",
    "            team_mask = (tracked_people.class_id == current_team_id)\n",
    "            # Get detections and labels for this team/role\n",
    "            team_detections = tracked_people[team_mask]\n",
    "            team_labels = [label for i, label in enumerate(final_labels) if team_mask[i]]\n",
    "\n",
    "            if len(team_detections) == 0: continue # Skip if no detections for this team\n",
    "\n",
    "            # Get the dynamic color calculated earlier for this team/role ID\n",
    "            team_color = dynamic_color_map.get(current_team_id, FALLBACK_COLOR) # Use fallback if ID unexpected\n",
    "\n",
    "            # Create temporary annotators with the specific color for this group\n",
    "            temp_ellipse_annotator = sv.EllipseAnnotator(color=team_color, thickness=ELLIPSE_THICKNESS)\n",
    "            temp_label_annotator = sv.LabelAnnotator(\n",
    "                color=team_color, text_color=LABEL_TEXT_COLOR,\n",
    "                text_position=LABEL_TEXT_POSITION, text_scale=LABEL_TEXT_SCALE,\n",
    "                text_thickness=LABEL_TEXT_THICKNESS\n",
    "            )\n",
    "            # Annotate ellipses and labels for this team/role group\n",
    "            try:\n",
    "                annotated_frame = temp_ellipse_annotator.annotate(annotated_frame, team_detections)\n",
    "                annotated_frame = temp_label_annotator.annotate(annotated_frame, team_detections, team_labels)\n",
    "            except Exception as e:\n",
    "                # Log errors during annotation phase\n",
    "                print(f\"[Frame {frame_idx}] Error during annotation for team {current_team_id}: {e}\")\n",
    "\n",
    "    # Return the fully annotated frame\n",
    "    return annotated_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36ee85b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Main Execution Function -----\n",
    "def main(args):\n",
    "    # Make global state variables modifiable within this function\n",
    "    global player_data, recently_lost_jerseys, ball_positions, detection_model, team_classifier\n",
    "\n",
    "    # --- Print Configuration ---\n",
    "    print(\"--- Configuration ---\")\n",
    "    print(f\"Source Video: {args.source}\")\n",
    "    print(f\"Annotated Video Output: {args.output}\")\n",
    "    print(f\"Action Frames Output Dir: {args.actions_dir}\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    print(f\"ReID Weights Path: {args.reid_weights}\")\n",
    "    print(f\"OCR Enabled: {PADDLEOCR_AVAILABLE}\")\n",
    "    print(f\"Groq API Key Loaded: {'Yes' if GROQ_API_KEY else 'No (VLM Analysis Disabled)'}\")\n",
    "    print(f\"Groq VLM Model: {GROQ_VLM_MODEL if GROQ_API_KEY else 'N/A'}\")\n",
    "    print(\"--------------------\")\n",
    "\n",
    "\n",
    "    # --- Create Output Directories ---\n",
    "    os.makedirs(os.path.dirname(args.output), exist_ok=True)\n",
    "    os.makedirs(args.actions_dir, exist_ok=True)\n",
    "    if PADDLEOCR_AVAILABLE: os.makedirs(OCR_DEBUG_DIR, exist_ok=True)\n",
    "\n",
    "    # --- Load Models ---\n",
    "    # In a real scenario, load your trained models here\n",
    "    print(\"Loading models...\")\n",
    "    try:\n",
    "        # Replace MockModel and MockTeamClassifier with your actual loading logic\n",
    "        # Example: detection_model = load_yolo(args.model_path)\n",
    "        # Example: team_classifier = load_classifier(args.classifier_path)\n",
    "        detection_model = MockModel() # Using placeholder\n",
    "        team_classifier = MockTeamClassifier() # Using placeholder\n",
    "        print(\"Models loaded (using placeholders).\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading models: {e}. Exiting.\")\n",
    "        exit(1)\n",
    "\n",
    "    # --- Get Video Info ---\n",
    "    print(\"Getting video info...\")\n",
    "    try:\n",
    "        # Use supervision to get video properties\n",
    "        video_info = sv.VideoInfo.from_video_path(str(args.source))\n",
    "        width, height, fps = video_info.width, video_info.height, video_info.fps\n",
    "        total_frames = video_info.total_frames if video_info.total_frames else 0\n",
    "        if not fps or fps <= 0: # Handle cases where FPS might be missing/invalid\n",
    "             print(\"Warning: Invalid FPS from supervision. Trying OpenCV.\")\n",
    "             raise ValueError(\"Invalid FPS\")\n",
    "    except Exception as e_sv:\n",
    "        print(f\"Warning: Could not get video info via supervision ({e_sv}). Using OpenCV fallback.\")\n",
    "        try:\n",
    "            cap = cv2.VideoCapture(str(args.source))\n",
    "            if not cap.isOpened(): raise IOError(f\"Cannot open video file: {args.source}\")\n",
    "            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "            fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            cap.release()\n",
    "            if not fps or fps <= 0: fps = 30; print(\"Warning: OpenCV also failed to get valid FPS. Assuming 30 FPS.\")\n",
    "            if total_frames <= 0: total_frames = 0; print(\"Warning: Total frames unknown.\")\n",
    "            # Create a VideoInfo object for VideoSink based on OpenCV results\n",
    "            video_info = sv.VideoInfo(width=width, height=height, fps=fps)\n",
    "        except Exception as e_cv:\n",
    "             print(f\"FATAL: Error getting video info via OpenCV fallback: {e_cv}. Exiting.\")\n",
    "             exit(1)\n",
    "\n",
    "    print(f\"Video Info: {width}x{height}, FPS: {fps:.2f}, Total Frames: {total_frames if total_frames > 0 else 'Unknown'}\")\n",
    "\n",
    "    # --- Initialize Tracker, InteractionTracker, Ball Trail ---\n",
    "    print(\"Initializing trackers...\")\n",
    "    reid_weights_path = Path(args.reid_weights)\n",
    "    tracker = BotSort(\n",
    "        reid_weights=reid_weights_path if reid_weights_path.exists() else None,\n",
    "        device=DEVICE,\n",
    "        half=False, # Adjust based on your model/GPU (True for FP16)\n",
    "        with_reid=reid_weights_path.exists(),\n",
    "        frame_rate=fps if fps > 0 else 30 # Provide frame rate to tracker\n",
    "    )\n",
    "    print(f\"BoTSORT initialized. ReID enabled: {reid_weights_path.exists()}\")\n",
    "\n",
    "    interaction_tracker = InteractionTracker(fps=fps)\n",
    "    print(f\"InteractionTracker initialized. Buffer size: {interaction_tracker.frame_buffer.maxlen} frames.\")\n",
    "\n",
    "    # Initialize ball trail deque\n",
    "    if fps > 0:\n",
    "        trail_maxlen = int(fps * BALL_TRAIL_SECONDS)\n",
    "        ball_positions = deque(maxlen=trail_maxlen)\n",
    "        print(f\"Ball trail deque initialized with maxlen={trail_maxlen}\")\n",
    "    else:\n",
    "        ball_positions = deque(maxlen=1) # Minimal deque if FPS unknown\n",
    "        print(\"Warning: FPS unknown. Ball trail length may be inaccurate.\")\n",
    "\n",
    "    # Update global LOST_TRACK_MEMORY_FRAMES based on actual FPS\n",
    "    global LOST_TRACK_MEMORY_FRAMES\n",
    "    LOST_TRACK_MEMORY_FRAMES = int(fps * LOST_TRACK_MEMORY_SECONDS) if fps > 0 else 30 * LOST_TRACK_MEMORY_SECONDS\n",
    "    print(f\"Lost track memory set to {LOST_TRACK_MEMORY_FRAMES} frames ({LOST_TRACK_MEMORY_SECONDS} seconds)\")\n",
    "\n",
    "    # --- Initialize Groq Client (if API key is available) ---\n",
    "    groq_client = None\n",
    "    if GROQ_API_KEY:\n",
    "        try:\n",
    "            groq_client = Groq(api_key=GROQ_API_KEY)\n",
    "            print(\"Groq client initialized.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing Groq client: {e}. VLM analysis will be disabled.\")\n",
    "            GROQ_API_KEY = None # Disable Groq if client fails\n",
    "    else:\n",
    "        print(\"GROQ_API_KEY not set. VLM analysis will be disabled.\")\n",
    "\n",
    "    # --- Video Processing Loop ---\n",
    "    print(\"Starting video processing loop...\")\n",
    "    frame_generator = sv.get_video_frames_generator(source_path=str(args.source), stride=1)\n",
    "    # Use VideoSink for efficient video writing\n",
    "    with sv.VideoSink(target_path=args.output, video_info=video_info) as sink:\n",
    "        # Setup progress bar if total frames are known\n",
    "        tqdm_total = total_frames if total_frames and total_frames > 0 else None\n",
    "        with tqdm(total=tqdm_total, desc=\"Processing video\", unit=\"frame\", mininterval=1.0) as pbar:\n",
    "            for frame_idx, frame in enumerate(frame_generator):\n",
    "                if frame is None:\n",
    "                    print(f\"\\nWarning: Received None frame at index {frame_idx}, ending processing.\")\n",
    "                    break # Stop processing if a frame is None\n",
    "                try:\n",
    "                    # Process the current frame\n",
    "                    annotated_frame = process_frame(frame, frame_idx, tracker, interaction_tracker, width, height)\n",
    "                    # Write the *annotated* frame to the output video file\n",
    "                    sink.write_frame(frame=annotated_frame)\n",
    "\n",
    "                except Exception as e:\n",
    "                    # Catch errors during frame processing\n",
    "                    print(f\"\\n--- CRITICAL ERROR processing frame {frame_idx}: {e} ---\")\n",
    "                    traceback.print_exc() # Print detailed traceback\n",
    "                    print(\"Attempting to continue processing...\")\n",
    "                    # Optionally write the original frame to avoid stopping video output\n",
    "                    # sink.write_frame(frame=frame)\n",
    "                # Update progress bar\n",
    "                if pbar is not None: pbar.update(1)\n",
    "\n",
    "    # --- Post-Processing: Save Interaction Frames & Analyze with Groq ---\n",
    "    print(f\"\\nFinished processing video frames.\")\n",
    "    confirmed_interactions_count = len(interaction_tracker.confirmed_interactions)\n",
    "    print(f\"Found {confirmed_interactions_count} confirmed interactions.\")\n",
    "\n",
    "    if confirmed_interactions_count > 0:\n",
    "        print(\"Saving interaction frame sequences and performing VLM analysis (if enabled)...\")\n",
    "        saved_clips_count = 0\n",
    "        analyzed_clips_count = 0\n",
    "        # Iterate through all confirmed interactions using their index\n",
    "        for idx in range(confirmed_interactions_count):\n",
    "            # Retrieve frames for this specific interaction index\n",
    "            # This also marks the interaction as processed in the tracker\n",
    "            clip_frames_data = interaction_tracker.get_clip_frames_data(idx)\n",
    "\n",
    "            if clip_frames_data:\n",
    "                 # Get interaction details for naming and analysis\n",
    "                 interaction = interaction_tracker.confirmed_interactions[idx]\n",
    "                 interaction_info = {\n",
    "                     'index': idx,\n",
    "                     'ball_id': interaction['ball_id'], # Note: This is the placeholder ID (-1)\n",
    "                     'player_id': interaction['player_id'] # This is the player's tracker ID\n",
    "                 }\n",
    "                 # Save the frames as images, get the directory path if successful\n",
    "                 saved_clip_path = save_interaction_frames(\n",
    "                     clip_frames_data, args.actions_dir, interaction_info, fps\n",
    "                 )\n",
    "\n",
    "                 if saved_clip_path:\n",
    "                     saved_clips_count += 1\n",
    "                     # --- Call Groq API if enabled and save was successful ---\n",
    "                     if groq_client:\n",
    "                         try:\n",
    "                             analyze_action_frames(\n",
    "                                 clip_dir=saved_clip_path,\n",
    "                                 groq_client=groq_client,\n",
    "                                 player_id=interaction_info['player_id'],\n",
    "                                 ball_id=interaction_info['ball_id']\n",
    "                             )\n",
    "                             analyzed_clips_count += 1\n",
    "                         except Exception as e_groq:\n",
    "                             print(f\"Error during Groq analysis for {saved_clip_path}: {e_groq}\")\n",
    "                 # else: Saving frames failed (error already printed by save_interaction_frames)\n",
    "            # else: Frame data retrieval failed (error already printed by get_clip_frames_data)\n",
    "\n",
    "        print(f\"\\nFinished saving frame sequences for {saved_clips_count} interactions to '{args.actions_dir}'.\")\n",
    "        if GROQ_API_KEY:\n",
    "             print(f\"Attempted VLM analysis for {analyzed_clips_count} interactions.\")\n",
    "    else:\n",
    "        print(\"No interactions met the criteria for saving/analysis.\")\n",
    "\n",
    "    print(f\"Annotated video saved to: {args.output}\")\n",
    "\n",
    "    # --- Cleanup ---\n",
    "    if DEVICE.type == 'cuda':\n",
    "        try:\n",
    "            torch.cuda.empty_cache()\n",
    "            print(\"CUDA cache cleared.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error clearing CUDA cache: {e}\")\n",
    "    print(\"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a515968e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--source SOURCE] [--output OUTPUT]\n",
      "                             [--actions_dir ACTIONS_DIR]\n",
      "                             [--reid_weights REID_WEIGHTS]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/run/user/1000/jupyter/runtime/kernel-v382f4ee6a98fac3bcf9fffa63abbf8064afb62cbb.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Advanced Football Tracking with Action Frame Extraction and VLM Analysis\")\n",
    "    parser.add_argument(\"--source\", type=str, default=DEFAULT_SOURCE_VIDEO_PATH, help=\"Path to the source video file.\")\n",
    "    parser.add_argument(\"--output\", type=str, default=DEFAULT_OUTPUT_VIDEO_PATH, help=\"Path to save the annotated output video.\")\n",
    "    parser.add_argument(\"--actions_dir\", type=str, default=DEFAULT_ACTIONS_DIR, help=\"Directory to save action frame sequences.\")\n",
    "    parser.add_argument(\"--reid_weights\", type=str, default=DEFAULT_REID_WEIGHTS_PATH, help=\"Path to the ReID model weights for BoTSORT (e.g., clip_market1501.pt).\")\n",
    "    # Add arguments for your actual model paths if needed, e.g.:\n",
    "    # parser.add_argument(\"--model_path\", type=str, required=True, help=\"Path to detection model weights.\")\n",
    "    # parser.add_argument(\"--classifier_path\", type=str, required=True, help=\"Path to team classifier model weights.\")\n",
    "\n",
    "    # In a notebook environment, you might prefer to define args manually:\n",
    "    # class Args:\n",
    "    #     source = \"path/to/your/video.mp4\"\n",
    "    #     output = \"output/annotated_video.mp4\"\n",
    "    #     actions_dir = \"output/action_frames\"\n",
    "    #     reid_weights = \"models/clip_market1501.pt\"\n",
    "    #     # model_path = \"models/yolo.pt\"\n",
    "    #     # classifier_path = \"models/classifier.pt\"\n",
    "    # args = Args()\n",
    "\n",
    "    # Parse arguments if running as a script\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # --- Run the main processing function ---\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949f04fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "football",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
